{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdallahyx/Face-Verification-and-Recognition/blob/main/Face_Verification_and_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LQIK4qStSXg"
      },
      "source": [
        "# Face Verification and Recognition\n",
        "\n",
        "Building a face verification and recognition system.\n",
        "\n",
        "Many of the ideas presented here are from [FaceNet](https://arxiv.org/pdf/1503.03832.pdf).\n",
        "\n",
        "**Face Verification** a 1:1 matching problem.\n",
        "\n",
        "**Face Recognition** a 1:K matching problem.\n",
        "\n",
        "FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, you can then determine if two pictures are of the same person.\n",
        "\n",
        "\n",
        "* Implementing one-shot learning to solve a face recognition problem\n",
        "* Applying the triplet loss function to learn a network's parameters in the context of face recognition\n",
        "* Mapping face images into 128-dimensional encodings using a pretrained model\n",
        "* Performing face verification and face recognition with these encodings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZIcD6ArtSXo"
      },
      "source": [
        "\n",
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z-yggKHatSXo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import PIL\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAIeiBZFtSXp"
      },
      "source": [
        "\n",
        "## Face Verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPm2JTKtSXq"
      },
      "source": [
        "\n",
        "## Encoding Face Images into a 128-Dimensional Vector\n",
        "\n",
        "\n",
        "\n",
        "### Using a ConvNet to Compute Encodings\n",
        "\n",
        "Loading FaceNet pre-trained model.\n",
        "\n",
        "The network architecture follows the Inception model from [Szegedy *et al*..](https://arxiv.org/abs/1409.4842) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/a-m-k-18/Face-Recognition-System/blob/master/facenet_keras.h5?raw=true"
      ],
      "metadata": {
        "id": "qNSJFzuFta4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0v_oBrJctSXq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "json_file = open('/content/model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.load_weights('/content/facenet_keras.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdr9GOv8tSXr",
        "outputId": "8f120b61-5d69-4dee-be70-81bf4eace3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
            "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
          ]
        }
      ],
      "source": [
        "print(model.inputs)\n",
        "print(model.outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFGlSmQktSXs"
      },
      "source": [
        "\n",
        "### The Triplet Loss\n",
        "\n",
        "\n",
        "Training will use triplets of images $(A, P, N)$:\n",
        "\n",
        "- A is an \"Anchor\" image--a picture of a person.\n",
        "- P is a \"Positive\" image--a picture of the same person as the Anchor image.\n",
        "- N is a \"Negative\" image--a picture of a different person than the Anchor image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f05732f7068382cb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "C9ao0NG4tSXt"
      },
      "outputs": [],
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss \n",
        "    \n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
        "            positive -- the encodings for the positive images, of shape (None, 128)\n",
        "            negative -- the encodings for the negative images, of shape (None, 128)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    \n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "    \n",
        "\n",
        "    # Compute the (encoding) distance between the anchor and the positive\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis = -1)\n",
        "    # Compute the (encoding) distance between the anchor and the negative\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis = -1)\n",
        "    # subtract the two previous distances and add alpha.\n",
        "    basic_loss = tf.maximum(tf.add(tf.subtract(pos_dist,neg_dist),alpha),0)\n",
        "    # Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
        "    loss = tf.reduce_sum(basic_loss)\n",
        "\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0faw5qMtSXu"
      },
      "source": [
        "\n",
        "\n",
        "## Loading the Pre-trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-953bcab8e9bbba10",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aT4-aGRJtSXu"
      },
      "outputs": [],
      "source": [
        "FRmodel = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPJF2CZjtSXv"
      },
      "source": [
        "\n",
        "## Applying the Model\n",
        "\n",
        "\n",
        "### Face Verification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fBRouYbutSXv"
      },
      "outputs": [],
      "source": [
        "def img_to_encoding(image_path, model):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(160, 160))\n",
        "    img = np.around(np.array(img) / 255.0, decimals=12)\n",
        "    x_train = np.expand_dims(img, axis=0)\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding / np.linalg.norm(embedding, ord=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7yDZXwdtSXw"
      },
      "source": [
        "### Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ba2f317e79e15a2f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c3x-1j6rtSXw"
      },
      "outputs": [],
      "source": [
        "def verify(image_path, identity, database, model):\n",
        "    \"\"\"\n",
        "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
        "    \n",
        "    Arguments:\n",
        "        image_path -- path to an image\n",
        "        identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
        "        database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
        "        model -- your Inception model instance in Keras\n",
        "    \n",
        "    Returns:\n",
        "        dist -- distance between the image_path and the image of \"identity\" in the database.\n",
        "        door_open -- True, if the door should open. False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the encoding for the image.\n",
        "    encoding = img_to_encoding(image_path,model)\n",
        "    # Compute distance with identity's image \n",
        "    dist = np.linalg.norm(encoding-database[identity])\n",
        "    if dist < 0.7:\n",
        "        print(\"It's \" + str(identity))\n",
        "        door_open = True\n",
        "    else:\n",
        "        print(\"It's not \" + str(identity))\n",
        "        door_open = False\n",
        "        \n",
        "    return dist, door_open"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ZgRE-dtSXw"
      },
      "source": [
        "\n",
        "### Face Recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a04ff2b5fd1186f8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "eDBr4eWgtSXx"
      },
      "outputs": [],
      "source": [
        "def face_recognition(image_path, database, model):\n",
        "    \"\"\"\n",
        "    Implements face recognition by finding who is the person on the image_path image.\n",
        "    \n",
        "    Arguments:\n",
        "        image_path -- path to an image\n",
        "        database -- database containing image encodings along with the name of the person on the image\n",
        "        model -- your Inception model instance in Keras\n",
        "    \n",
        "    Returns:\n",
        "        min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
        "        identity -- string, the name prediction for the person on image_path\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    ## Compute the target \"encoding\" for the image.\n",
        "    encoding =  img_to_encoding(image_path,model)\n",
        "    \n",
        "    ## Find the closest encoding\n",
        "    \n",
        "    # Initialize \"min_dist\" to a large value\n",
        "    min_dist = 100\n",
        "    \n",
        "    # Loop over the database dictionary's names and encodings.\n",
        "    for (name, db_enc) in database.items():\n",
        "        \n",
        "        # Compute L2 distance between the target \"encoding\" and the current db_enc from the database.\n",
        "        dist = np.linalg.norm(encoding - db_enc)\n",
        "\n",
        "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name.\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "    \n",
        "    if min_dist > 0.7:\n",
        "        print(\"Not in the database.\")\n",
        "    else:\n",
        "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
        "        \n",
        "    return min_dist, identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1BwBLv2tSXx"
      },
      "source": [
        "<a name='6'></a>\n",
        "## 6 - References\n",
        "1. Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
        "\n",
        "2. Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)\n",
        "\n",
        "3. This implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet\n",
        "\n",
        "4. Further inspiration was found here: https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/\n",
        "\n",
        "5. And here: https://github.com/nyoki-mtl/keras-facenet/blob/master/notebook/tf_to_keras.ipynb"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}